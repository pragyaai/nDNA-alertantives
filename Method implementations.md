# Spectral Curvature, Thermodynamic Length, and Belief Vector Fields

This markdown file describes the three .ipynb notebooks in each of the folders: Spectral Curvature, Themrodynamic Length and Belief Vector Field each with the same name. They are end-to-end Colab-ready notebooks that measures these three geometry-of-inference diagnostics inside transformer language models: spectral curvature along depth on the Fisher–Rao sphere, thermodynamic length computed in two complementary ways, and belief vector fields that describe how training signals would nudge the model’s next-token beliefs at each layer. The implementation runs on decoder-only architectures and has been exercised on `microsoft/DialoGPT-medium`, `gpt2`, and `TinyLlama/TinyLlama-1.1B-Chat-v1.0`. All components are written with attention to numerical stability and GPU efficiency, and the code paths are adapted where GPT-2-style blocks differ from LLaMA-style blocks.

Let us call these three separate notebooks as three-parts of a larger notebook that computes these three different metrics. The first part computes spectral curvature across depth for a small set of multilingual and mathematical prompts. At each depth ℓ we expose the model’s “logit-lens” distribution over the next token, map it to the Fisher–Rao sphere via the square-root embedding, and then approximate the intrinsic curvature of the depth-parametrized belief path using projected finite differences that live in the correct tangent spaces. The second part computes thermodynamic length in terms of model parameters as well as in terms of model predictions. In the parameter view we measure per-layer observed Fisher via the mean squared norm of the log-likelihood score restricted to a single block using a logit-lens loss; in the prediction view we compute exact Fisher–Rao steps between consecutive layerwise next-token distributions and sum them along depth. The third part constructs belief vector fields by pushing the standard next-token cross-entropy signal through softmax and through the square-root map onto the sphere, projecting to the tangent space, and averaging the resulting tangents over a concept-conditioned dataset. The output is a clean layer-profile of how strongly and in which intrinsic direction the training signal tries to steer beliefs for each semantic concept.

The spectral curvature section starts from the sequence of layerwise distributions $p^{(\ell)}_ t$ $\in$ $\Delta^{V-1}$ for a fixed position $t$. It embeds them as $u_{\ell} = \sqrt{p^{(\ell)}_ t}$ $\in$ $\mathbb{S}^{V-1}$, ensures tangent-space computations by removing the radial component with the orthogonal projector $\Pi_\ell = I - u_\ell u_\ell^\top$, defines the discrete speed $s_\ell = \|\Pi_\ell (u_{\ell+1} - u_\ell)\|_ 2$, and then forms the projected second difference $\Delta^2 u_\ell = \Pi_\ell (u_{\ell+1} - 2u_\ell + u_{\ell-1})$. The curvature surrogate is

$$
\kappa_\ell^{(\text{simp})} \;=\; \frac{\big\|\Delta^2 u_\ell\big\|_ 2}{\big(s_\ell^2 + \varepsilon\big)^{3/2}}, \qquad \varepsilon>0,
$$

which is the discrete analogue of $\|\ddot{u}\|/\|\dot{u}\|^3$ and is robust to local reparameterization. The notebook provides code to collect the layerwise next-token distributions using a logit lens at every block, converts them to $u_\ell$, computes the projected finite differences, and plots $\kappa_\ell^{(\text{simp})}$ across depth for the chosen prompts.

The thermodynamic length section implements two complementary definitions. The parameter-space definition measures the block-wise observed Fisher as the expected squared norm of the layer-restricted score $s_\ell(x)$, obtained by detaching the incoming hidden state before each block, applying the shared LM head on the block output to form a logit-lens loss, and then backpropagating once to gather all per-block gradients in one pass. The statistic $L_\ell = \sum_{x\in\mathcal D}\|s_\ell(x)\|^2$ is reported per layer after averaging over a dataset. The prediction-space definition treats layer index as a path parameter through distribution space. Between two consecutive layers $\ell$ and $\ell+1$ at a supervised position we compute the exact Fisher–Rao step $2\arccos\Big(\sum_i \sqrt{p^{(\ell)}_ i p^{(\ell+1)}_ i}\Big)$. Numerically we implement this via the Bhattacharyya coefficient in log-space. If $\log p$ and $\log q$ are the layerwise log-probabilities, the coefficient is $\mathrm{BC}(p,q) = \sum_i \sqrt{p_i q_i} = \sum_i \exp\big(\tfrac{1}{2}\log p_i + \tfrac{1}{2}\log q_i\big)$. We compute $\log \mathrm{BC} = \mathrm{logsumexp}\big(\tfrac{1}{2}(\log p + \log q)\big)$ and then the Fisher–Rao step $2\arccos(\mathrm{BC})$. Summing or averaging these steps across positions gives a smooth profile of belief motion per inter-layer transition. The notebook also includes a small “semantic efficiency” fusion that rescales both the parameter-strain curve and the belief-change curve onto a common range and reports $\log\big(1+\Delta p_\ell/E_\ell\big)$ as an efficiency proxy.

The belief vector field section defines the per-example tangent push at a given depth as the intrinsic effect of moving logits in the direction that increases the log-likelihood under teacher forcing. If $z_{\ell,t}$ are the logits, $q_{\ell,t}=\mathrm{softmax}(z_{\ell,t}/\tau)$ the distribution, $u_{\ell,t}=\sqrt{q_{\ell,t}}$ the sphere embedding, and $g_{\ell,t} = \nabla_{z_{\ell,t}} \log q_{\ell,t}(y_t^\star) = \tfrac{1}{\tau}\,(e_{y_t^\star}-q_{\ell,t})$ the logit-space gradient, then the pushforward to the sphere is

$$
t_{\ell,t}(x) \;=\; \frac{1}{2\tau}\;\Pi_{\ell,t}\;\mathrm{Diag}\!\big(u_{\ell,t}(x)\big)\;g_{\ell,t}(x) \;\in\; T_{u_{\ell,t}}\mathbb{S}^{V-1}.
$$

This vector lies in the tangent plane at $u_{\ell,t}$ and is an FR-consistent representation of the local “belief pressure” exerted by the loss at that depth. Averaging these tangents over positions and examples for a concept-conditioned dataset yields a single vector per layer whose Euclidean norm on the sphere corresponds (up to the fixed factor two) to the Fisher–Rao magnitude of that pressure. The notebook computes and plots these layerwise magnitudes for three AG-News concepts and for each model.

The datasets used are chosen to be large enough for smooth estimates while remaining quick to run in Colab. SQuAD is used to provide diverse, content-rich prompts for parameter-space observed Fisher and for prediction-space Fisher–Rao steps, using a simple prompt template that concatenates the question, context, and an “Answer:” prefix. AG News is used to define three contrasting concept classes—World, Sports, and Sci/Tech—so belief vector fields can be compared across semantically distinct conditions. For curvature diagnostics a small list of carefully chosen prompts in English, Hindi, Sanskrit, Arabic, and mathematical LaTeX is included to show how geometry changes with linguistic and stylistic variation.

There are a few architectural notes that matter in practice. GPT-2 family models expose blocks under `model.transformer.h` and use the final layer norm `model.transformer.ln_f` before the shared `lm_head`. LLaMA family models expose decoder layers under `model.model.layers` and use `model.model.norm` before the `lm_head`. The notebook keeps the code paths separate where needed and uses `output_hidden_states=True` for the LLaMA forward call to obtain the per-layer hidden states in one pass. All logit-lens reads apply the correct final normalization before the head to mimic the top-of-stack readout at each depth. Mixed precision is enabled where stable, TF32 is allowed on Ampere/Hopper cards, and gradients are accumulated in a single backward pass for the observed Fisher section to minimize overhead.

The configuration knobs are defined at the top of each section to make experiments straightforward. Sequence length and batch size control memory footprint. The number of samples and the number of supervised positions per example govern the smoothness of the curves and the runtime. The temperature parameter $\tau$ appears explicitly in the belief-vector derivations; for probing it is standard to keep $\tau=1$ unless you are explicitly studying calibration or smoothing effects. Small epsilons are used to stabilize probability floors and curvature denominators. The curvature computation explicitly projects finite differences to the tangent space to avoid spurious radial components and to ensure comparisons are intrinsic to the Fisher–Rao geometry.

The notebook produces three sets of figures. The first is a grid of spectral curvature profiles versus layer index for the selected prompts. The second reports the thermodynamic length two ways: a per-layer observed-Fisher effort curve and the exact Fisher–Rao step length between consecutive layers, accompanied by a combined “semantic efficiency” figure that shows how efficiently internal parameter strain translates into belief motion. The third presents layerwise belief-vector magnitudes for three semantic concepts per model, which can be read as how strongly each depth level “wants” to adjust its predicted next-token beliefs for that concept.

Running the notebook requires a recent `transformers`, `datasets`, `torch`, and `accelerate` stack, and a GPU-backed Colab runtime is recommended. The code initializes all randomness for reproducibility, sets conservative defaults for sequence lengths and batch sizes for stability, and opts out of unnecessary caches in the LLaMA path to keep memory use predictable. If you adapt the concept datasets or switch to different tokenizers, ensure that the logit-lens application remains consistent with the model’s top-of-stack normalization and that supervised positions are selected in the same teacher-forced manner so that belief pushes remain comparable across layers.

If you use or extend this work, consider citing the classical Fisher–Rao geometry and the square-root embedding equivalence to spherical geodesics. The isometry $d_{\mathrm{FR}}(p,q) = 2\arccos\big(\sum_i \sqrt{p_i q_i}\big)$ is the backbone of both the thermodynamic length and belief-vector constructions, and is what allows the analysis to be intrinsic, coordinate-free, and robust across tokenizations and languages.
